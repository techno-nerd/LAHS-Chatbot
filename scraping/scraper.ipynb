{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN = \"https://lahs.mvla.net/\"\n",
    "LINK_CLASS_NAME = \"dropdown-item\"\n",
    "SAVE_DIR = \"../documents/\"\n",
    "MAX_DOCS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Helper functions for dealing with the different files\n",
    "\n",
    "def read_urls(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [line.strip() for line in file.readlines()]\n",
    "\n",
    "\n",
    "def write_urls(file_path, urls):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for url in urls:\n",
    "            file.write(f\"{url}\\n\")\n",
    "\n",
    "\n",
    "def save_doc(url, content):\n",
    "    file_path = url.replace('http://', '').replace('https://', '').replace('/', '_') + '.txt'\n",
    "    file_path = os.path.join(SAVE_DIR, file_path)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = read_urls(\"done.txt\")\n",
    "pending = read_urls(\"pending.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "def is_valid(link):\n",
    "    if 'http' in link and DOMAIN not in link: #External link\n",
    "        return False\n",
    "    elif 'pdf' in link: #Not a webpage\n",
    "        return False\n",
    "    elif \"javascript\" in link:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def get_links(soup):\n",
    "    for a_tag in soup.find_all('a', class_=LINK_CLASS_NAME):\n",
    "        link = a_tag.get('href')\n",
    "        if is_valid(link):\n",
    "            if 'http' not in link:\n",
    "                link = urljoin(DOMAIN, link)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if link in done or link in pending:\n",
    "            continue\n",
    "        else:\n",
    "            pending.append(link)\n",
    "\n",
    "    write_urls(\"pending.txt\", pending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        get_links(soup)\n",
    "        \n",
    "        for nav in soup.find_all(class_='top-bar-header'):\n",
    "            nav.decompose()\n",
    "        \n",
    "        save_doc(url, soup.get_text(separator='\\n', strip=True))\n",
    "        done.append(url)\n",
    "        write_urls(\"done.txt\", done)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(pending) != 0 and len(os.listdir(SAVE_DIR)) < MAX_DOCS:\n",
    "    url = pending.pop(0)\n",
    "    scrape_page(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
